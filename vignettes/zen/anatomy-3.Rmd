---
title: "Anatomy of an R Project: File Structure"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{anatomy-3}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo=F, message=F}
library(metrics.in.r)
library(here)
library(ggplot2)
library(broom)
```

### The Pattern

File structure can be a non-trivial thing in projects. Badly structured projects are confusing—you don't know what comes from where and can't tell heads from tails. File names are meaningful, and the folders you put them in are meaningful. In our code, we can create a consistent way to refer to them. This is the focus of this article. I will give a philosophy of file organization, and a specific example that I have used before. It is from a replication assignment, and you can find it [here](https://github.com/nateybear/causal-inference-2022). The example is not intended to be the be-all-end-all of what you should do. Take it more as a guideline for your work.

### Data

If you're running a research project, you probably are using some data. If you are using data, you probably will need to clean it somehow. Keep your data in a folder called "data." If you need to clean it, write an R script inside of a folder called "R" and a subfolder that is reserved for data cleaning/building tasks. In my example below I call this folder "build."

![](https://raw.githubusercontent.com/nateybear/causal-inference-2022/main/writing/rdemo_assets/build-directory.png "Showing the correspondence between data names and the R scripts that build them")

The name of the R script corresponds to the name of the data set that it cleans. It reads in a "raw" data file, applies whatever transformations are necessary to arrive at a "clean" dataset, and then writes the clean dataset to a new file. The point is to exactly document the steps to get from a dataset that you find on the Internet or in the wild to the dataset that you use in your regressions.

### Tables and Figures

If you're writing a report, you probably have figures and tables that you need to include. You write R scripts that read in the cleaned data and generate some output as a `.png` (for figures) or `.tex` (for tables). As with data, your figures and tables live in clearly marked folders. The R scripts that generate the figures or tables have names that correspond to the figures or tables that they create. In my example project, I made a subfolder in the R directory called "output" that has code to generate both tables and figures:

![](https://raw.githubusercontent.com/nateybear/causal-inference-2022/main/writing/rdemo_assets/describe-directory.png "Correspondence between names of figures and tables and names or R scripts")
When you're creating a figure, you are reading in a cleaned dataset, possibly doing some cleaning, and then generating a graph. I strongly recommend using the [ggplot2](https://ggplot2.tidyverse.org/) package. You can then use the `ggsave()` function to write your graph as a png file.

For tables, you will often have a data frame that represents the table that you want to put in your paper. The [`kableExtra`](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_pdf.pdf) package has utilities to generate a pretty LaTeX table from a data frame, and then you can write to a file using the tidyverse function `write_lines()`.

Other times, you may want to output a regression summary into a table. You can either use the `tidy()` function from the `broom` package to turn your summary into a data frame, and then follow the `kableExtra` steps above; or you can use the [`stargazer`](https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf) package to generate pretty regression output. The trade-off between the two is that because stargazer gives you so much out-of-the-box functionality, it's naturally harder to configure it for a very specific use case.

### Where are my files? `here` they are.

We've talked a lot about reading and writing files in this article, so obviously there is a little bit of nuance that I want to impart on you. There are a few different ways that we can refer to files in our code.

The first is an **absolute** path to our file. This tells the computer how to find the file from the very top of the filesystem. For example, I could refer to a data file in my project as `/Users/nate/workspace/my-project/data/some_dataset.csv`. You can tell a path is an absolute path because it begins with a forward slash, `/`. Absolute paths can also start with a tilde, `~`, which indicates that the location is relative to the home directory of the user. So, on my computer, `~/workspace/my-project/data/some_dataset.csv` and `/Users/nate/workspace/my-project/data/some_dataset.csv` are the same thing.

Why are absolute file paths bad? Because they only work on your computer. If some else runs your code, they will have to change the path of the file to wherever their project is located on their own computer. There are better ways to do this. You should avoid absolute paths at all costs, and you will lose points on your assignments if I see an absolute path in your code.

The second way to refer to files is via a **relative** path. This tells the computer how to find the file *relative* to where the script that is currently running lives. Relative paths can be identified because they start with `./` and `../`. If my code to clean the `some_dataset.csv` file above lives in `~/workspace/my-project/R/build/some_dataset.R`, then I can refer to the dataset in my R code by writing `../../data/some_dataset.csv`. That means, "Go up one folder, go up another folder, go into the data folder, and find some_dataset.csv there." Two dots, `../`, refers to your script's parent folder, and one dot, `./`, refers to the folder that it is currently in. In most cases, the one dot can be implicit: hence, `my_script.R` and `./my_script.R` refer to the same thing.

Why are relative file paths bad? They certainly do work on multiple computers because of their relative nature. However, they are prone to break when you reorganize your code. Say for example that you started out with all of your code in one `R` folder, but then later decide to make multiple subfolders, like `R/build` and `R/output`. When that happens, the path to your data file no longer becomes `../data/my_dataset.csv` and is now `../../data/my_dataset.csv`. To be clear this is not the worst thing in the world, and relative file paths are acceptable in many software engineering contexts. For our purposes, we can do one better, though.

The ideal way to refer to files in our projects is via a **project-relative** path. This tells the computer, "Look for the closest `.Rproj` file, then look relative to that folder for my file." This is ideal because it both runs on multiple computers and is robust to you reorganizing your R scripts. The way we do this in R couldn't be easier. We load the `here` library, then use the `here()` function whenever we refer to a file. So, the dataset that we have been working with this entire time could be referred to with `here("data/my_dataset.csv")`. If is still a little abstract for you, you can find plenty of usages in my example project that I linked. One instance is [here](https://github.com/nateybear/causal-inference-2022/blob/main/R/output/figure_arrests_by_racegender.R) in the code for generating a figure.

One gotcha to keep in mind with `here`: It is project-specific. So that means that if you don't have a project open in RStudio or you have the wrong project open, then `here` will not be able to find your files correctly. If you work with projects the way that I have suggested you should, this will not be an issue since you will always have a project open.

### Conclusion

Be militant with file naming. Follow a file structure that is *self-evident*. This fits perfectly with the "consistent and predictable" mantra from my meditation on programming. The `here` package gives you a consistent and predictable way to refer to files inside of your project.

### Bonus: Build Tools

It is more than enough to make your code so self-evident that someone else can figure out how to run your scripts and compile your project. To take it a step further, you can use a build tools such as a Makefile or a `targets` file. These are step-by-step recipes of how to build your projects.

The common vocabulary words that both of these tools use are "target" and "dependency." A target is something that you want to produce—think of your cleaned dataset, your figure, your LaTeX report. A dependency describes what the target depends on. Build tools track the dependencies so that targets only need to be rebuilt when a dependency changes. This means that you may have many scripts that you need to run that take a long time when all run together, but when using a build tool, it will only run the small parts of the project that *actually changed* each time.

A Makefile is a file-based build tool. This means that targets and dependencies are specified as files that exist in your project, and Make determines if a target needs to be rebuilt by comparing the last modified time of the target file with the last modified time of all of its dependencies. I prefer Makefiles to the `targets` package because it is a concept that almost all software engineers are familiar with. A good guide to Make for data scientists can be found [here](https://the-turing-way.netlify.app/reproducible-research/make.html).

An alternative to Make is the `targets` R package. Instead of being file-based, you write R code to describe your build process. `targets` determines whether a target needs to be rebuilt by comparing hashes (strings of letter which uniquely identify objects in code) of the dependencies. You can read about the `targets` package [here](https://books.ropensci.org/targets/).

Whichever build tool you use, one nice perk is that you can configure the Build tab in RStudio to work with your build tool. Then, you click the "Build All" button and your project is built! No more having to worry about running scripts in the right order.

If build tools sound good to you, then cool. It's beyond the scope of this article to go more into depth, but it's an interesting subject. If you're my student then feel free to reach out and talk about it.

<hr>

Happy Coding!
